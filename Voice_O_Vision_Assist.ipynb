{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nmB806m_35yH",
        "outputId": "4a2c742e-713c-4742-898c-a83471e05d9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting groq\n",
            "  Downloading groq-0.12.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from groq) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from groq) (0.27.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from groq) (2.9.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from groq) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.6)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq) (2.23.4)\n",
            "Downloading groq-0.12.0-py3-none-any.whl (108 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/108.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.9/108.9 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: groq\n",
            "Successfully installed groq-0.12.0\n",
            "Collecting faster-whisper\n",
            "  Downloading faster_whisper-1.0.3-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting av<13,>=11.0 (from faster-whisper)\n",
            "  Downloading av-12.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.6 kB)\n",
            "Collecting ctranslate2<5,>=4.0 (from faster-whisper)\n",
            "  Downloading ctranslate2-4.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.13 in /usr/local/lib/python3.10/dist-packages (from faster-whisper) (0.26.2)\n",
            "Requirement already satisfied: tokenizers<1,>=0.13 in /usr/local/lib/python3.10/dist-packages (from faster-whisper) (0.20.3)\n",
            "Collecting onnxruntime<2,>=1.14 (from faster-whisper)\n",
            "  Downloading onnxruntime-1.20.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from ctranslate2<5,>=4.0->faster-whisper) (75.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from ctranslate2<5,>=4.0->faster-whisper) (1.26.4)\n",
            "Requirement already satisfied: pyyaml<7,>=5.3 in /usr/local/lib/python3.10/dist-packages (from ctranslate2<5,>=4.0->faster-whisper) (6.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13->faster-whisper) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13->faster-whisper) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13->faster-whisper) (24.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13->faster-whisper) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13->faster-whisper) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13->faster-whisper) (4.12.2)\n",
            "Collecting coloredlogs (from onnxruntime<2,>=1.14->faster-whisper)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime<2,>=1.14->faster-whisper) (24.3.25)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime<2,>=1.14->faster-whisper) (4.25.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime<2,>=1.14->faster-whisper) (1.13.1)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime<2,>=1.14->faster-whisper)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.13->faster-whisper) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.13->faster-whisper) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.13->faster-whisper) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.13->faster-whisper) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime<2,>=1.14->faster-whisper) (1.3.0)\n",
            "Downloading faster_whisper-1.0.3-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading av-12.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.5/33.5 MB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ctranslate2-4.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.4/38.4 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.20.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (13.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: humanfriendly, ctranslate2, av, coloredlogs, onnxruntime, faster-whisper\n",
            "Successfully installed av-12.3.0 coloredlogs-15.0.1 ctranslate2-4.5.0 faster-whisper-1.0.3 humanfriendly-10.0 onnxruntime-1.20.0\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libportaudio2 libportaudiocpp0\n",
            "Suggested packages:\n",
            "  portaudio19-doc\n",
            "The following NEW packages will be installed:\n",
            "  libportaudio2 libportaudiocpp0 portaudio19-dev\n",
            "0 upgraded, 3 newly installed, 0 to remove and 49 not upgraded.\n",
            "Need to get 188 kB of archives.\n",
            "After this operation, 927 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libportaudio2 amd64 19.6.0-1.1 [65.3 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libportaudiocpp0 amd64 19.6.0-1.1 [16.1 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 portaudio19-dev amd64 19.6.0-1.1 [106 kB]\n",
            "Fetched 188 kB in 1s (195 kB/s)\n",
            "Selecting previously unselected package libportaudio2:amd64.\n",
            "(Reading database ... 123629 files and directories currently installed.)\n",
            "Preparing to unpack .../libportaudio2_19.6.0-1.1_amd64.deb ...\n",
            "Unpacking libportaudio2:amd64 (19.6.0-1.1) ...\n",
            "Selecting previously unselected package libportaudiocpp0:amd64.\n",
            "Preparing to unpack .../libportaudiocpp0_19.6.0-1.1_amd64.deb ...\n",
            "Unpacking libportaudiocpp0:amd64 (19.6.0-1.1) ...\n",
            "Selecting previously unselected package portaudio19-dev:amd64.\n",
            "Preparing to unpack .../portaudio19-dev_19.6.0-1.1_amd64.deb ...\n",
            "Unpacking portaudio19-dev:amd64 (19.6.0-1.1) ...\n",
            "Setting up libportaudio2:amd64 (19.6.0-1.1) ...\n",
            "Setting up libportaudiocpp0:amd64 (19.6.0-1.1) ...\n",
            "Setting up portaudio19-dev:amd64 (19.6.0-1.1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "Collecting pyaudio\n",
            "  Downloading PyAudio-0.2.14.tar.gz (47 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.1/47.1 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: pyaudio\n",
            "  Building wheel for pyaudio (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyaudio: filename=PyAudio-0.2.14-cp310-cp310-linux_x86_64.whl size=63860 sha256=66ad895f24b8ba27357c3d8c18cdc71e99e92960896d9667b0567c36b88a9beb\n",
            "  Stored in directory: /root/.cache/pip/wheels/d6/21/f4/0b51d41ba79e51b16295cbb096ec49f334792814d545b508c5\n",
            "Successfully built pyaudio\n",
            "Installing collected packages: pyaudio\n",
            "Successfully installed pyaudio-0.2.14\n",
            "Collecting speechrecognition\n",
            "  Downloading SpeechRecognition-3.11.0-py2.py3-none-any.whl.metadata (28 kB)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from speechrecognition) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from speechrecognition) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->speechrecognition) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->speechrecognition) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->speechrecognition) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->speechrecognition) (2024.8.30)\n",
            "Downloading SpeechRecognition-3.11.0-py2.py3-none-any.whl (32.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.8/32.8 MB\u001b[0m \u001b[31m54.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: speechrecognition\n",
            "Successfully installed speechrecognition-3.11.0\n"
          ]
        }
      ],
      "source": [
        "!pip install groq\n",
        "!pip install faster-whisper\n",
        "!apt-get install portaudio19-dev\n",
        "!pip install pyaudio\n",
        "!pip install speechrecognition\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3pov_VSv_raR",
        "outputId": "e6ad05cf-10d0-4820-ddff-d3a4f5e9b745"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (11.0.0)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.10/dist-packages (3.0.3)\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.1-py3-none-any.whl.metadata (8.3 kB)\n",
            "Collecting flask-cors\n",
            "  Downloading Flask_Cors-5.0.0-py2.py3-none-any.whl.metadata (5.5 kB)\n",
            "Requirement already satisfied: Werkzeug>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from flask) (3.1.3)\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.10/dist-packages (from flask) (3.1.4)\n",
            "Requirement already satisfied: itsdangerous>=2.1.2 in /usr/local/lib/python3.10/dist-packages (from flask) (2.2.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.10/dist-packages (from flask) (8.1.7)\n",
            "Requirement already satisfied: blinker>=1.6.2 in /usr/local/lib/python3.10/dist-packages (from flask) (1.9.0)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.1.2->flask) (3.0.2)\n",
            "Downloading pyngrok-7.2.1-py3-none-any.whl (22 kB)\n",
            "Downloading Flask_Cors-5.0.0-py2.py3-none-any.whl (14 kB)\n",
            "Installing collected packages: pyngrok, flask-cors\n",
            "Successfully installed flask-cors-5.0.0 pyngrok-7.2.1\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "E: Unable to locate package ngrok\n",
            "Collecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub\n",
            "Successfully installed pydub-0.25.1\n"
          ]
        }
      ],
      "source": [
        "!pip install Pillow\n",
        "\n",
        "!pip install flask pyngrok flask-cors\n",
        "\n",
        "!apt-get install ffmpeg\n",
        "\n",
        "!apt-get install ngrok\n",
        "\n",
        "!pip install pydub\n",
        "#!ngrok http 5000\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z5fqlF_jbrQf"
      },
      "outputs": [],
      "source": [
        "from flask import Flask, request, jsonify\n",
        "from flask_cors import CORS\n",
        "from pyngrok import ngrok\n",
        "import os\n",
        "import re\n",
        "from groq import Groq\n",
        "from PIL import ImageGrab, Image\n",
        "import cv2\n",
        "import pyperclip\n",
        "import google.generativeai as genai\n",
        "from openai import OpenAI\n",
        "import pyaudio\n",
        "from faster_whisper import WhisperModel\n",
        "import speech_recognition as sr\n",
        "import time\n",
        "import json\n",
        "from PIL import Image\n",
        "\n",
        "# Initialize Flask app and CORS\n",
        "app = Flask(__name__)\n",
        "CORS(app)\n",
        "\n",
        "# Set up ngrok\n",
        "ngrok.set_auth_token('')  # Replace with your actual auth token\n",
        "public_url = ngrok.connect(5000)\n",
        "print(\" * ngrok tunnel \\\"{}\\\" -> \\\"http://127.0.0.1:5000\\\"\".format(public_url))\n",
        "\n",
        "# Initialize models and settings\n",
        "groq_client = Groq(api_key=\"\")  # Replace with your Groq API key\n",
        "genai.configure(api_key='')  # Replace with your Generative AI API key\n",
        "openai_client = OpenAI(api_key='')  # Replace with your OpenAI API key\n",
        "whisper_model = WhisperModel(\"base\", device='cpu', compute_type='int8')\n",
        "\n",
        "# System message for AI\n",
        "sys_msg=(\n",
        "    'You are a multi-modal AI voice assistant. Your user may or may not have attached a photo for context '\n",
        "    '(either a screenshot or a webcam capture). Any photo has already been processed into a hoghly detailed '\n",
        "    'text prompt that will be attached to their transcribed voice prompt. Generate the most useful and '\n",
        "    'factual response possible, carefully considering all previous generated text in your response before '\n",
        "    'adding new tokens to the response. Do not expect or request images, just use the context if added. '\n",
        "    'Use all of the context of this conversation so your response is relevant to the conversation. Make '\n",
        "    'your responses clear and concise, avoiding any verbosity.'\n",
        ")\n",
        "\n",
        "convo = [{'role': 'system', 'content': sys_msg}]\n",
        "\n",
        "\n",
        "generation_config = {\n",
        "    'temperature': 0.7,\n",
        "    'top_p': 1,\n",
        "    'top_k': 1,\n",
        "    'max_output_tokens': 2048\n",
        "}\n",
        "\n",
        "safety_settings = [\n",
        "    {\n",
        "        'category': 'HARM_CATEGORY_HARASSMENT',\n",
        "        'threshold': 'BLOCK_NONE'\n",
        "    },\n",
        "    {\n",
        "        'category': 'HARM_CATEGORY_HATE_SPEECH',\n",
        "        'threshold': 'BLOCK_NONE'\n",
        "    },\n",
        "    {\n",
        "        'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT',\n",
        "        'threshold': 'BLOCK_NONE'\n",
        "    },\n",
        "    {\n",
        "        'category': 'HARM_CATEGORY_DANGEROUS_CONTENT',\n",
        "        'threshold': 'BLOCK_NONE'\n",
        "    },\n",
        "]\n",
        "\n",
        "model = genai.GenerativeModel('gemini-1.5-flash-latest',\n",
        "                              generation_config=generation_config,\n",
        "                              safety_settings=safety_settings)\n",
        "\n",
        "num_cores = os.cpu_count()\n",
        "whisper_size = 'base'\n",
        "whisper_model = WhisperModel(\n",
        "    whisper_size,\n",
        "    device='cpu',\n",
        "    compute_type='int8',\n",
        "    cpu_threads=num_cores // 2,\n",
        "    num_workers=num_cores // 2\n",
        ")\n",
        "\n",
        "def groq_prompt(prompt, img_context):\n",
        "    if img_context:\n",
        "        prompt = f'USER PROMPT: {prompt}\\n\\n  IMAGE CONTEXT: {img_context}'\n",
        "    convo.append({'role': 'user', 'content': prompt})\n",
        "    chat_completion = groq_client.chat.completions.create(messages=convo, model='llama3-70b-8192')\n",
        "    response = chat_completion.choices[0].message\n",
        "    convo.append(response)\n",
        "    return response.content\n",
        "\n",
        "'''\n",
        "def take_screenshot():\n",
        "    path = 'screenshot.jpg'\n",
        "    screenshot = ImageGrab.grab()\n",
        "    rgb_screenshot = screenshot.convert('RGB')\n",
        "    rgb_screenshot.save(path, quality=15)\n",
        "\n",
        "def web_cam_capture():\n",
        "    if not web_cam.isOpened():\n",
        "        print('Error: Camera did not open successfully')\n",
        "        exit()\n",
        "    path = 'webcam.jpg'\n",
        "    ret, frame = web_cam.read()\n",
        "    cv2.imwrite(path, frame)\n",
        "'''\n",
        "def get_clipboard_text():\n",
        "    clipboard_content = pyperclip.paste()\n",
        "    if isinstance(clipboard_content, str):\n",
        "        return clipboard_content\n",
        "    else:\n",
        "        print('No Clipboard text to copy')\n",
        "        return None\n",
        "\n",
        "def vision_prompt(prompt, photo_path):\n",
        "    img = Image.open(photo_path)\n",
        "    prompt = (\n",
        "        'You are the vision analysis AI that provides semantic meaning from images to provide context '\n",
        "        'to send to another AI that will create a response to the user. Do not respond as the AI assistant '\n",
        "        'to the user. Instead take the user prompt input and try to extract all meaning from the photo '\n",
        "        'relevant to the user prompt. Then generate as much objective data about the image for the AI '\n",
        "        f'assistant who will respond to the user. \\nUSER PROMPT: {prompt}'\n",
        "    )\n",
        "    response = model.generate_content([prompt, img])\n",
        "    return response.text\n",
        "\n",
        "def function_call(prompt):\n",
        "    sys_msg = (\n",
        "        'You are an AI function calling model. You will determine whether extracting the user\\'s clipboard content, '\n",
        "        'taking a screenshot, capturing the webcam, or calling no functions is best for a voice assistant to respond '\n",
        "        'to the user\\'s prompt. The webcam can be assumed to be a normal laptop webcam facing the user. You will '\n",
        "        'respond with only one selection from this list: [\"extract clipboard\", \"take screenshot\", \"capture webcam\", \"None\"] \\n'\n",
        "        'function call name exactly as I listed.'\n",
        "    )\n",
        "\n",
        "    function_convo = [{'role': 'system', 'content': sys_msg},\n",
        "                      {'role': 'user', 'content': prompt}]\n",
        "    chat_completion = groq_client.chat.completions.create(messages=function_convo, model='llama3-70b-8192')\n",
        "    response = chat_completion.choices[0].message\n",
        "    return response.content\n",
        "\n",
        "def wav_to_text(audio_path):\n",
        "    segments, _ = whisper_model.transcribe(audio_path)\n",
        "    text = ''.join(segment.text for segment in segments)\n",
        "    return text\n",
        "\n",
        "def extract_prompt(transcribed_text, wake_word):\n",
        "    pattern = rf'\\b{re.escape(wake_word)}[\\s,.?!]*([A-Za-z0-9].*)'\n",
        "    match = re.search(pattern, transcribed_text, re.IGNORECASE)\n",
        "\n",
        "    if match:\n",
        "        prompt = match.group(1).strip()\n",
        "        return prompt\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# Function to process audio and generate responses\n",
        "def process_audio_file(audio_file_path):\n",
        "    # Read and transcribe the audio file\n",
        "    print('Processing audio file:', audio_file_path)\n",
        "\n",
        "    prompt_text = wav_to_text(audio_file_path)\n",
        "    print('Transcribed text:', prompt_text)\n",
        "\n",
        "    clean_prompt = extract_prompt(prompt_text, \"hey\")\n",
        "    print('Clean prompt:', clean_prompt)\n",
        "\n",
        "    if clean_prompt:\n",
        "        call = function_call(clean_prompt)\n",
        "        print('Function call response:', call)\n",
        "\n",
        "        visual_context = None  # Initialize visual context variable\n",
        "        '''\n",
        "        if 'take screenshot' in call:\n",
        "            take_screenshot()\n",
        "            visual_context = vision_prompt(prompt=clean_prompt, photo_path='screenshot.jpg')\n",
        "        elif 'extract clipboard' in call:\n",
        "            paste = get_clipboard_text()\n",
        "            prompt = f'{clean_prompt}\\n\\n CLIPBOARD CONTENT: {paste}'\n",
        "        '''\n",
        "        response = groq_prompt(prompt=clean_prompt, img_context=visual_context)\n",
        "        return response\n",
        "    else:\n",
        "        print('No valid clean prompt found.')\n",
        "        return None\n",
        "\n",
        "@app.route(\"/upload\", methods=[\"POST\"])\n",
        "def upload_audio():\n",
        "    if \"file\" not in request.files:\n",
        "        return jsonify({\"error\": \"No file part\"}), 400\n",
        "\n",
        "    file = request.files[\"file\"]\n",
        "    if file.filename == '':\n",
        "        return jsonify({\"error\": \"No selected file\"}), 400\n",
        "\n",
        "    # Save the uploaded file temporarily\n",
        "    file.save(\"uploaded_audio.wav\")\n",
        "\n",
        "    # Process the audio file and return the response\n",
        "    response = process_audio_file(\"uploaded_audio.wav\")\n",
        "\n",
        "    if response:\n",
        "        return jsonify({\"response\": response})\n",
        "    else:\n",
        "        return jsonify({\"error\": \"Could not process audio\"}), 500\n",
        "\n",
        "# Add new endpoint to process the image file\n",
        "@app.route(\"/upload_image\", methods=[\"POST\"])\n",
        "def upload_image():\n",
        "    if \"file\" not in request.files:\n",
        "        return jsonify({\"error\": \"No file part\"}), 400\n",
        "\n",
        "    file = request.files[\"file\"]\n",
        "    if file.filename == '':\n",
        "        return jsonify({\"error\": \"No selected file\"}), 400\n",
        "\n",
        "    # Save the uploaded image temporarily\n",
        "    image_path = \"uploaded_image.jpg\"\n",
        "    file.save(image_path)\n",
        "\n",
        "    # Process the image file (for example, using your vision_prompt function)\n",
        "    visual_context = vision_prompt(prompt=\"Analyze this image\", photo_path=image_path)\n",
        "\n",
        "    if visual_context:\n",
        "        return jsonify({\"response\": visual_context})\n",
        "    else:\n",
        "        return jsonify({\"error\": \"Could not process image\"}), 500\n",
        "\n",
        "@app.route(\"/upload_screenshot\", methods=[\"POST\"])\n",
        "def upload_screenshot():\n",
        "    if \"file\" not in request.files:\n",
        "        print(\"No file part in the screenshot request\")\n",
        "        return jsonify({\"error\": \"No file part\"}), 400\n",
        "\n",
        "    file = request.files[\"file\"]\n",
        "    if file.filename == '':\n",
        "        print(\"No selected screenshot file\")\n",
        "        return jsonify({\"error\": \"No selected file\"}), 400\n",
        "\n",
        "    # Save the uploaded screenshot temporarily\n",
        "    screenshot_path = \"uploaded_screenshot.jpg\"\n",
        "    file.save(screenshot_path)\n",
        "    print(\"Screenshot saved successfully:\", screenshot_path)\n",
        "\n",
        "    # Process the screenshot using vision_prompt\n",
        "    visual_context = vision_prompt(prompt=\"Analyze this screenshot\", photo_path=screenshot_path)\n",
        "    print(\"Screenshot processing response:\", visual_context)\n",
        "\n",
        "    if visual_context:\n",
        "        return jsonify({\"response\": visual_context})\n",
        "    else:\n",
        "        return jsonify({\"error\": \"Could not process screenshot\"}), 500\n",
        "\n",
        "# Start Flask app\n",
        "if __name__ == \"__main__\":\n",
        "    app.run()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}